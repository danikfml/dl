{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, gc, math, random, sqlite3, ujson, numpy as np, torch\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Set, Tuple, Iterable\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom torch.utils.data import DataLoader\nimport faiss\n\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\n@dataclass\nclass Cfg:\n    lang: str = \"rus\"\n    corpus_limit_docs: int = 120_000\n    max_chars: int = 1000\n    subset_queries: int = 20_000\n    bi_model: str = \"intfloat/multilingual-e5-base\"\n    bi_batch: int = 16\n    bi_epochs: int = 2\n    bi_max_len: int = 192\n    topk: int = 200\n    xe_model: str = \"BAAI/bge-reranker-v2-m3\"\n    xe_batch: int = 2\n    xe_epochs: int = 1\n    xe_max_len: int = 128\n    xe_cap: int = 6000\n    hn_neg_per_pos: int = 1\n    rerank_topk: int = 20\n\ncfg = Cfg()\ncfg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:24.694288Z","iopub.execute_input":"2025-11-08T17:47:24.694517Z","iopub.status.idle":"2025-11-08T17:47:39.434971Z","shell.execute_reply.started":"2025-11-08T17:47:24.694496Z","shell.execute_reply":"2025-11-08T17:47:39.434317Z"}},"outputs":[{"name":"stderr","text":"2025-11-08 17:47:31.597093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762624051.619409     223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762624051.626234     223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"Cfg(lang='rus', corpus_limit_docs=120000, max_chars=1000, subset_queries=20000, bi_model='intfloat/multilingual-e5-base', bi_batch=16, bi_epochs=2, bi_max_len=192, topk=200, xe_model='BAAI/bge-reranker-v2-m3', xe_batch=2, xe_epochs=1, xe_max_len=128, xe_cap=6000, hn_neg_per_pos=1, rerank_topk=20)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"HF_REPO_ID = \"PaDaS-Lab/webfaq-retrieval\"\nHF_REVISION = \"main\"\n\ndef hf_path(lang: str, filename: str) -> str:\n    return hf_hub_download(repo_id=HF_REPO_ID, repo_type=\"dataset\",\n                           filename=f\"{lang}/{filename}\", revision=HF_REVISION,\n                           local_dir=None, local_dir_use_symlinks=True)\n\ndef iter_jsonl(local_path: str) -> Iterable[dict]:\n    with open(local_path, \"rb\") as f:\n        for raw in f:\n            line = raw.strip()\n            if not line:\n                continue\n            try:\n                yield ujson.loads(line.decode(\"utf-8\"))\n            except:\n                yield ujson.loads(line)\n\ndef load_queries(lang: str, split: str, limit: int | None = None) -> Dict[str, str]:\n    p = hf_path(lang, f\"queries-{split}.jsonl\")\n    items = []\n    for row in iter_jsonl(p):\n        qid = str(row.get(\"_id\") or row.get(\"id\"))\n        q = (row.get(\"text\") or row.get(\"query\") or \"\").strip()\n        if q:\n            items.append((qid, q))\n    random.shuffle(items)\n    if limit:\n        items = items[:limit]\n    return dict(items)\n\ndef load_qrels(lang: str, split: str) -> Dict[str, Set[str]]:\n    p = hf_path(lang, f\"qrels-{split}.jsonl\")\n    m: Dict[str, Set[str]] = {}\n    for row in iter_jsonl(p):\n        qid = str(row.get(\"query-id\") or row.get(\"_id\") or row.get(\"qid\"))\n        did = str(row.get(\"corpus-id\") or row.get(\"doc_id\") or row.get(\"document_id\") or row.get(\"pid\"))\n        if not qid or not did:\n            continue\n        if qid not in m:\n            m[qid] = set()\n        m[qid].add(did)\n    return m\n\ndef build_sqlite_corpus(lang: str, out_path: str, limit_docs: int, max_chars: int) -> Tuple[str, int]:\n    p = hf_path(lang, \"corpus.jsonl\")\n    con = sqlite3.connect(out_path)\n    cur = con.cursor()\n    cur.execute(\"DROP TABLE IF EXISTS docs\")\n    cur.execute(\"CREATE TABLE docs (id TEXT PRIMARY KEY, text TEXT)\")\n    con.commit()\n    added = 0\n    batch = []\n    with open(p, \"rb\") as f:\n        for raw in f:\n            if not raw.strip():\n                continue\n            row = ujson.loads(raw.decode(\"utf-8\"))\n            did = str(row.get(\"_id\") or row.get(\"doc_id\") or row.get(\"corpus-id\") or row.get(\"id\"))\n            text = (row.get(\"text\") or \"\").strip()\n            title = (row.get(\"title\") or \"\").strip()\n            if title and title.lower() != \"text\":\n                text = (title + \" \" + text).strip()\n            if max_chars:\n                text = text[:max_chars]\n            batch.append((did, text))\n            if len(batch) >= 1000:\n                cur.executemany(\"INSERT OR REPLACE INTO docs(id,text) VALUES(?,?)\", batch)\n                con.commit()\n                added += len(batch)\n                batch = []\n                if added >= limit_docs:\n                    break\n    if batch and added < limit_docs:\n        cur.executemany(\"INSERT OR REPLACE INTO docs(id,text) VALUES(?,?)\", batch)\n        con.commit()\n        added += len(batch)\n    cur.close()\n    con.close()\n    return out_path, added\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:39.435697Z","iopub.execute_input":"2025-11-08T17:47:39.436295Z","iopub.status.idle":"2025-11-08T17:47:39.448317Z","shell.execute_reply.started":"2025-11-08T17:47:39.436259Z","shell.execute_reply":"2025-11-08T17:47:39.447687Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def _dcg(scores: List[int]) -> float:\n    s = 0.0\n    for i, rel in enumerate(scores, start=1):\n        if rel > 0:\n            s += (2 ** rel - 1) / math.log2(i + 1)\n    return s\n\ndef _idcg(n: int) -> float:\n    return _dcg([1] * n) if n > 0 else 0.0\n\ndef recall_at_k(run: Dict[str, List[str]], qrels: Dict[str, Set[str]], k: int = 10) -> float:\n    a = b = 0\n    for qid, rels in qrels.items():\n        b += 1\n        top = run.get(qid, [])[:k]\n        a += 1 if any(d in rels for d in top) else 0\n    return a / b if b else 0.0\n\ndef mrr_at_k(run: Dict[str, List[str]], qrels: Dict[str, Set[str]], k: int = 10) -> float:\n    s = 0.0\n    n = 0\n    for qid, rels in qrels.items():\n        n += 1\n        top = run.get(qid, [])[:k]\n        rr = 0.0\n        for i, d in enumerate(top, start=1):\n            if d in rels:\n                rr = 1.0 / i\n                break\n        s += rr\n    return s / n if n else 0.0\n\ndef ndcg_at_k(run: Dict[str, List[str]], qrels: Dict[str, Set[str]], k: int = 10) -> float:\n    s = 0.0\n    n = 0\n    for qid, rels in qrels.items():\n        n += 1\n        top = run.get(qid, [])[:k]\n        gains = [1 if d in rels else 0 for d in top]\n        idcg = _idcg(min(len(rels), k))\n        s += (_dcg(gains) / (idcg if idcg > 0 else 1.0))\n    return s / n if n else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:39.449102Z","iopub.execute_input":"2025-11-08T17:47:39.449394Z","iopub.status.idle":"2025-11-08T17:47:39.474340Z","shell.execute_reply.started":"2025-11-08T17:47:39.449376Z","shell.execute_reply":"2025-11-08T17:47:39.473640Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"urls_ok = True\nsqlite_path, added = build_sqlite_corpus(cfg.lang, \"corpus.sqlite\", cfg.corpus_limit_docs, cfg.max_chars)\ncon = sqlite3.connect(sqlite_path)\ncur = con.cursor()\nfetch = lambda did: (cur.execute(\"SELECT text FROM docs WHERE id=?\", (did,)).fetchone() or [\"\"])[0]\n\nqueries_tr = load_queries(cfg.lang, \"train\", limit=cfg.subset_queries)\nqueries_te = load_queries(cfg.lang, \"test\", limit=None)\nqrels_tr = load_qrels(cfg.lang, \"train\")\nqrels_te = load_qrels(cfg.lang, \"test\")\n\nlen(queries_tr), len(queries_te), added\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:39.474966Z","iopub.execute_input":"2025-11-08T17:47:39.475200Z","iopub.status.idle":"2025-11-08T17:47:46.314443Z","shell.execute_reply.started":"2025-11-08T17:47:39.475178Z","shell.execute_reply":"2025-11-08T17:47:46.313814Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(20000, 10000, 120000)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import random, gc, torch\ncfg.bi_model   = \"intfloat/multilingual-e5-small\"\ncfg.bi_epochs  = 1\ncfg.bi_max_len = 128\ncfg.bi_batch   = 32\nBI_CAP         = 6000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:46.316438Z","iopub.execute_input":"2025-11-08T17:47:46.316722Z","iopub.status.idle":"2025-11-08T17:47:46.320416Z","shell.execute_reply.started":"2025-11-08T17:47:46.316702Z","shell.execute_reply":"2025-11-08T17:47:46.319737Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nimport os, gc, random, torch\nfrom torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import get_linear_schedule_with_warmup\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\n\nBI_CAP         = getattr(cfg, \"bi_cap\", 6000)\nMODEL_NAME     = cfg.bi_model\nEPOCHS         = cfg.bi_epochs\nMAX_LEN        = cfg.bi_max_len  \nINIT_BS        = cfg.bi_batch   \nLR             = 2e-5\nMAX_NORM       = 1.0     \nSEED           = 42\n\ndef format_e5(text: str, is_query: bool) -> str:\n    return (\"query: \" if is_query else \"passage: \") + text.strip()\n\nrandom.seed(SEED)\npairs = []\nfor qid, q in queries_tr.items():\n    rel = qrels_tr.get(qid, [])\n    if not rel:\n        continue\n    did = next(iter(rel))\n    t = fetch(did)\n    if t:\n        pairs.append((q, t))\nrandom.shuffle(pairs)\npairs = pairs[:BI_CAP]\nprint(f\"pairs={len(pairs)}\")\n\nclass PairDataset(Dataset):\n    def __init__(self, pairs):\n        self.ex = [InputExample(texts=[format_e5(q, True), format_e5(p, False)]) for q, p in pairs]\n    def __len__(self): return len(self.ex)\n    def __getitem__(self, idx): return self.ex[idx]\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = True\nprint(\"device:\", device)\n\nmodel = SentenceTransformer(MODEL_NAME, device=device)\nmodel.max_seq_length = MAX_LEN\nloss_fn = losses.MultipleNegativesRankingLoss(model)\n\ndef train_with_bs(batch_size: int) -> bool:\n    try:\n        ds = PairDataset(pairs)\n        dl = DataLoader(ds, shuffle=True, batch_size=batch_size, collate_fn=model.smart_batching_collate, drop_last=True)\n        t_total = len(dl) * EPOCHS\n        optimizer = AdamW(model.parameters(), lr=LR)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1, int(0.1*t_total)), num_training_steps=t_total)\n\n        global_step = 0\n        for epoch in range(EPOCHS):\n            model.train()\n            pbar = tqdm(dl, desc=f\"Epoch {epoch+1}/{EPOCHS} | bs={batch_size}\", leave=True)\n            for features, labels in pbar:\n                features = [{k: v.to(device) for k, v in f.items()} for f in features]\n                labels = labels.to(device)\n\n                loss = loss_fn(features, labels)\n                loss.backward()\n                clip_grad_norm_(model.parameters(), MAX_NORM)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n                global_step += 1\n                pbar.set_postfix(loss=float(loss.detach().cpu()))\n            pbar.close()\n        return True\n    except RuntimeError as e:\n        msg = str(e).lower()\n        if (\"out of memory\" in msg or (\"cuda\" in msg and \"memory\" in msg)) and device == \"cuda\":\n            torch.cuda.empty_cache()\n            return False\n        raise\n\nbs = int(INIT_BS)\nwhile bs >= 8:\n    print(f\"try batch_size={bs}\")\n    ok = train_with_bs(bs)\n    if ok:\n        break\n    print(f\"OOM → batch_size: {bs} → {max(8, bs//2)}\")\n    bs = max(8, bs // 2)\n\nif bs < 8:\n    raise RuntimeError(\"Не удалось подобрать batch_size ≥ 8\")\n\nbi = model\ngc.collect()\nlen(pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:47:46.321200Z","iopub.execute_input":"2025-11-08T17:47:46.321488Z","iopub.status.idle":"2025-11-08T17:48:31.294402Z","shell.execute_reply.started":"2025-11-08T17:47:46.321464Z","shell.execute_reply":"2025-11-08T17:48:31.293732Z"}},"outputs":[{"name":"stdout","text":"pairs=6000\ndevice: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"try batch_size=32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/1 | bs=32:   0%|          | 0/187 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2781717cb534f5ca7e114e84ac6dfb6"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6000"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import numpy as np, faiss, gc, torch\nfrom tqdm.auto import tqdm\n\ndef l2norm(x: np.ndarray) -> np.ndarray:\n    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n    return x / n\n\nif torch.cuda.is_available():\n    try:\n        bi.to(\"cuda\")\n    except Exception:\n        pass\n\nDOC_CAP = getattr(cfg, \"doc_cap\", None)\nids_all = [r[0] for r in cur.execute(\"SELECT id FROM docs\").fetchall()]\nids = ids_all[:DOC_CAP] if DOC_CAP else ids_all\n\nsample_id = ids[0]\ndim = bi.encode([format_e5(fetch(sample_id), False)],\n                batch_size=1, convert_to_numpy=True, show_progress_bar=False).shape[1]\n\nindex = faiss.IndexFlatIP(dim)\n\nB = getattr(cfg, \"index_batch\", 256 if torch.cuda.is_available() else 64)\n\nprint(f\"Indexing on device={'cuda' if torch.cuda.is_available() else 'cpu'} | docs={len(ids)} | dim={dim} | start_batch={B}\")\npbar = tqdm(total=len(ids), desc=\"Indexing (CUDA encode)\", leave=True)\n\ni = 0\nwhile i < len(ids):\n    curr = min(B, len(ids) - i)\n    try:\n        chunk = ids[i:i+curr]\n        texts = [format_e5(fetch(_id), False) for _id in chunk]\n        em = bi.encode(texts, batch_size=curr, convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n        em = l2norm(em)\n        index.add(em)\n        i += curr\n        pbar.update(curr)\n        del texts, em, chunk\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        if (i // max(1, B)) % 10 == 0:\n            gc.collect()\n    except RuntimeError as e:\n        msg = str(e).lower()\n        if (\"out of memory\" in msg or (\"cuda\" in msg and \"memory\" in msg)) and torch.cuda.is_available() and B > 16:\n            torch.cuda.empty_cache()\n            B = max(16, B // 2)\n            print(f\"OOM {i} → уменьшаю batch до {B}\")\n            continue\n        raise\npbar.close()\n\n(len(ids), dim, index.ntotal)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:48:31.295160Z","iopub.execute_input":"2025-11-08T17:48:31.295394Z","iopub.status.idle":"2025-11-08T17:51:33.332186Z","shell.execute_reply.started":"2025-11-08T17:48:31.295377Z","shell.execute_reply":"2025-11-08T17:51:33.331603Z"}},"outputs":[{"name":"stdout","text":"Indexing on device=cuda | docs=120000 | dim=384 | start_batch=256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Indexing (CUDA encode):   0%|          | 0/120000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2842ffdf1664dd1b7a0277cb33ad6eb"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(120000, 384, 120000)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nhn_pool = {}\nHN_BS = getattr(cfg, \"hn_batch\", max(32, cfg.bi_batch * 2))\nprint(f\"Build hard negatives | queries={len(queries_tr)} | hn_batch={HN_BS} | topk={cfg.topk}\")\n\ntr_qids = list(queries_tr.keys())\nfor i in tqdm(range(0, len(tr_qids), HN_BS), desc=\"Search train (CUDA)\"):\n    qb = tr_qids[i:i+HN_BS]\n    qt = [format_e5(queries_tr[q], True) for q in qb]\n    qe = bi.encode(qt, batch_size=HN_BS, convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n    qe = l2norm(qe)\n    _, I = index.search(qe, cfg.topk)\n    for j, idxs in enumerate(I):\n        cand = [ids[k] for k in idxs if k >= 0]\n        rel = qrels_tr.get(qb[j], set())\n        hn_pool[qb[j]] = [d for d in cand if d not in rel]\n    del qt, qe, I\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\nlen(hn_pool)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T17:52:37.519132Z","iopub.execute_input":"2025-11-08T17:52:37.519456Z","iopub.status.idle":"2025-11-08T17:53:16.033896Z","shell.execute_reply.started":"2025-11-08T17:52:37.519433Z","shell.execute_reply":"2025-11-08T17:53:16.032867Z"}},"outputs":[{"name":"stdout","text":"Build hard negatives | queries=20000 | hn_batch=64 | topk=200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Search train (CUDA):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb14b48cfd8545dc840efb4612bbc43d"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"20000"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import os, gc, math, torch, random\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.nn.utils import clip_grad_norm_\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import InputExample\n\ntry:\n    if torch.cuda.is_available() and 'bi' in globals():\n        bi.to('cpu'); torch.cuda.empty_cache()\nexcept Exception:\n    pass\n\ndef vram_info():\n    if not torch.cuda.is_available(): return (0,0)\n    free, total = torch.cuda.mem_get_info()\n    return (free//(1024**3), total//(1024**3))\n\nXE_MODEL_BIG   = getattr(cfg, \"xe_model\", \"BAAI/bge-reranker-v2-m3\")\nXE_MODEL_SMALL = \"ai-forever/ruBert-tiny2\"\nXE_EPOCHS      = int(getattr(cfg, \"xe_epochs\", 1))\nXE_MAX_LEN     = int(getattr(cfg, \"xe_max_len\", 128))\nXE_BATCH       = max(1, int(getattr(cfg, \"xe_batch\", 2)))\nXE_LR_BIG      = float(getattr(cfg, \"xe_lr\", 5e-6))\nXE_LR_SMALL    = 2e-5\nWARMUP_FR      = float(getattr(cfg, \"xe_warmup_fr\", 0.20))\nWEIGHT_DECAY   = float(getattr(cfg, \"xe_wd\", 0.01))\nGRAD_ACCUM     = int(getattr(cfg, \"xe_grad_accum\", 2))\nCLIP_NORM      = 1.0\nSAVE_DIR       = \"./outputs/crossencoder\"\nSEED           = 42\n\nrandom.seed(SEED); torch.manual_seed(SEED)\n\nif \"xe_examples\" not in globals() or not len(xe_examples):\n    raise RuntimeError(\"xe_examples пуст (сначала выполни ячейку 8)\")\n\npos = sum(1 for e in xe_examples if float(e.label) >= 0.5)\nneg = len(xe_examples) - pos\nprint(f\"XE examples: {len(xe_examples)} | pos={pos} neg={neg} | VRAM free/total: {vram_info()} GB\")\n\nclass XEDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_len: int):\n        self.ex = examples; self.tok = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.ex)\n    def __getitem__(self, i):\n        e = self.ex[i]; q, p = e.texts\n        enc = self.tok(q, p, truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        out = {k: v.squeeze(0) for k,v in enc.items()}\n        out[\"labels\"] = torch.tensor(float(e.label), dtype=torch.float32)\n        return out\n\ndef train_once(model_name, lr_init, max_len_init, bs_init):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else tokenizer.unk_token\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n    try: model.gradient_checkpointing_enable()\n    except Exception: pass\n\n    try:\n        model.to(device)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower() and device == \"cuda\":\n            torch.cuda.empty_cache()\n            return False, f\"OOM при переносе {model_name} на CUDA\"\n        raise\n\n    bs = int(bs_init)\n    ml = int(max_len_init)\n    lr = float(lr_init)\n    use_cuda = (device == \"cuda\")\n\n    pos_w = torch.tensor([max(1.0, neg / max(1, pos))], device=device if use_cuda else \"cpu\")\n    scaler = torch.cuda.amp.GradScaler(enabled=use_cuda)\n\n    while True:\n        try:\n            ds = XEDataset(xe_examples, tokenizer, ml)\n            dl = DataLoader(ds, shuffle=True, batch_size=bs, pin_memory=use_cuda)\n            total = max(1, len(dl) * XE_EPOCHS)\n            opt = AdamW([\n                {\"params\":[p for n,p in model.named_parameters() if p.requires_grad and not any(x in n for x in (\"bias\",\"LayerNorm.weight\",\"layer_norm.weight\",\"norm.weight\",\"ln.weight\"))], \"weight_decay\": WEIGHT_DECAY},\n                {\"params\":[p for n,p in model.named_parameters() if p.requires_grad and     any(x in n for x in (\"bias\",\"LayerNorm.weight\",\"layer_norm.weight\",\"norm.weight\",\"ln.weight\"))], \"weight_decay\": 0.0},\n            ], lr=lr)\n            warm = max(1, int(WARMUP_FR * total))\n            sch = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=total)\n            loss_fn = BCEWithLogitsLoss(pos_weight=pos_w.to(device if use_cuda else \"cpu\"))\n\n            print(f\"CrossEncoder train → model={model_name}, device={device}, bs={bs}, acc={GRAD_ACCUM}, \"\n                  f\"epochs={XE_EPOCHS}, max_len={ml}, steps/epoch={len(dl)}, lr={lr}, warmup={warm}\")\n\n            step_acc = 0\n            for ep in range(1, XE_EPOCHS+1):\n                model.train()\n                running = 0.0\n                pbar = tqdm(dl, desc=f\"XE Epoch {ep}/{XE_EPOCHS}\", leave=True)\n                for step, batch in enumerate(pbar, 1):\n                    batch = {k: v.to(device, non_blocking=use_cuda) for k,v in batch.items()}\n                    if use_cuda:\n                        with torch.cuda.amp.autocast():\n                            out = model(input_ids=batch[\"input_ids\"],\n                                        attention_mask=batch[\"attention_mask\"],\n                                        token_type_ids=batch.get(\"token_type_ids\", None))\n                            logits = out.logits.squeeze(-1)\n                            loss = loss_fn(logits, batch[\"labels\"]) / GRAD_ACCUM\n                        scaler.scale(loss).backward()\n                    else:\n                        out = model(input_ids=batch[\"input_ids\"],\n                                    attention_mask=batch[\"attention_mask\"],\n                                    token_type_ids=batch.get(\"token_type_ids\", None))\n                        logits = out.logits.squeeze(-1)\n                        loss = loss_fn(logits, batch[\"labels\"]) / GRAD_ACCUM\n                        loss.backward()\n\n                    step_acc += 1\n                    if step_acc % GRAD_ACCUM == 0:\n                        clip_grad_norm_(model.parameters(), CLIP_NORM)\n                        if use_cuda:\n                            scaler.step(opt); scaler.update()\n                        else:\n                            opt.step()\n                        opt.zero_grad(set_to_none=True); sch.step()\n\n                    running += float(loss.detach().cpu()) * GRAD_ACCUM\n                    if step % 10 == 0:\n                        pbar.set_postfix(loss=f\"{running/step:.4f}\")\n                pbar.close()\n\n            os.makedirs(SAVE_DIR, exist_ok=True)\n            model.save_pretrained(SAVE_DIR)\n            tokenizer.save_pretrained(SAVE_DIR)\n            print(f\"Saved to: {SAVE_DIR}\")\n            del dl, ds, model; gc.collect()\n            if use_cuda: torch.cuda.empty_cache()\n            return True, \"ok\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n            oom = (\"out of memory\" in msg) or (\"cuda\" in msg and \"memory\" in msg) or (\"same device\" in msg)\n            if oom and use_cuda:\n                torch.cuda.empty_cache()\n                if bs > 1:\n                    bs = max(1, bs // 2); print(f\"OOM →  batch до {bs}\"); continue\n                if ml > 64:\n                    ml = max(64, ml // 2); print(f\"OOM → max_len до {ml}\"); continue\n                return False, f\"OOM на CUDA при bs=1, max_len={ml}\"\n            raise\n\nok, msg = train_once(XE_MODEL_BIG, XE_LR_BIG, XE_MAX_LEN, XE_BATCH)\nif not ok and torch.cuda.is_available():\n    print(\"ruBert-tiny2 на CUDA для быстрого обучения:\", msg)\n    ok2, msg2 = train_once(XE_MODEL_SMALL, XE_LR_SMALL, min(96, XE_MAX_LEN), max(4, XE_BATCH))\n    if not ok2:\n        raise RuntimeError(f\"Не удалось обучить XE даже на компактной модели: {msg2}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T18:03:58.434117Z","iopub.execute_input":"2025-11-08T18:03:58.434437Z","iopub.status.idle":"2025-11-08T18:15:03.903124Z","shell.execute_reply.started":"2025-11-08T18:03:58.434416Z","shell.execute_reply":"2025-11-08T18:15:03.902215Z"}},"outputs":[{"name":"stdout","text":"XE examples: 6000 | pos=3000 neg=3000 | VRAM free/total: (13, 15) GB\nCrossEncoder train → model=BAAI/bge-reranker-v2-m3, device=cuda, bs=2, acc=2, epochs=1, max_len=128, steps/epoch=3000, lr=5e-06, warmup=600\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_223/4117862591.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=use_cuda)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"XE Epoch 1/1:   0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c654e01df52f47109e7382f4e9d907db"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_223/4117862591.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved to: ./outputs/crossencoder\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nrun_te = {}\nB_TEST = getattr(cfg, \"test_batch\", max(32, cfg.bi_batch * 2))\nprint(f\"Search test | queries={len(queries_te)} | batch={B_TEST} | topk={cfg.topk}\")\n\nte_qids = list(queries_te.keys())\nfor i in tqdm(range(0, len(te_qids), B_TEST), desc=\"Search test (CUDA)\"):\n    qb = te_qids[i:i+B_TEST]\n    qt = [format_e5(queries_te[q], True) for q in qb]\n    qe = bi.encode(qt, batch_size=B_TEST, convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n    qe = l2norm(qe)\n    _, I = index.search(qe, cfg.topk)\n    for j, idxs in enumerate(I):\n        run_te[qb[j]] = [ids[k] for k in idxs if k >= 0]\n    del qt, qe, I\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nbi_R10   = recall_at_k(run_te, qrels_te, 10)\nbi_MRR10 = mrr_at_k(run_te, qrels_te, 10)\nbi_nDCG10= ndcg_at_k(run_te, qrels_te, 10)\n{\"R@10\": bi_R10, \"MRR@10\": bi_MRR10, \"nDCG@10\": bi_nDCG10}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T18:15:27.952554Z","iopub.execute_input":"2025-11-08T18:15:27.952842Z","iopub.status.idle":"2025-11-08T18:17:53.087717Z","shell.execute_reply.started":"2025-11-08T18:15:27.952819Z","shell.execute_reply":"2025-11-08T18:17:53.087124Z"}},"outputs":[{"name":"stdout","text":"Search test | queries=10000 | batch=64 | topk=200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Search test (CUDA):   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb2273f839e48158ec357f98c5e4045"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'R@10': 0.2497, 'MRR@10': 0.2036152380952377, 'nDCG@10': 0.2147046798742635}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\nRERANK_TOPK = getattr(cfg, \"rerank_topk\", 20)\npred_bs      = getattr(cfg, \"rerank_batch\", 32)\n\nxe_path_or_name = \"./outputs/crossencoder\"\n\ndev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncross = CrossEncoder(xe_path_or_name, num_labels=1, device=dev, max_length=getattr(cfg, \"xe_max_len\", 128))\n\nreranked = {}\n\nprint(f\"Reranking → device={dev}, topk={RERANK_TOPK}, pred_batch={pred_bs}\")\nfor qid, cand_ids in tqdm(run_te.items(), desc=\"Reranking (CUDA)\"):\n    subset = cand_ids[:RERANK_TOPK]\n    pairs = [(queries_te[qid], fetch(cid)) for cid in subset]\n    b = pred_bs\n    while True:\n        try:\n            scores = cross.predict(pairs, batch_size=b).tolist()\n            break\n        except RuntimeError as e:\n            msg = str(e).lower()\n            if (\"out of memory\" in msg or (\"cuda\" in msg and \"memory\" in msg)) and dev == \"cuda\" and b > 4:\n                torch.cuda.empty_cache()\n                b = max(4, b // 2)\n                continue\n            raise\n    order = np.argsort(-np.array(scores))\n    reranked[qid] = [subset[i] for i in order]\n\npipe_R10   = recall_at_k(reranked, qrels_te, 10)\npipe_MRR10 = mrr_at_k(reranked, qrels_te, 10)\npipe_nDCG10= ndcg_at_k(reranked, qrels_te, 10)\n{\"R@10\": pipe_R10, \"MRR@10\": pipe_MRR10, \"nDCG@10\": pipe_nDCG10}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T18:17:56.999767Z","iopub.execute_input":"2025-11-08T18:17:57.000436Z","iopub.status.idle":"2025-11-08T18:56:29.721978Z","shell.execute_reply.started":"2025-11-08T18:17:57.000411Z","shell.execute_reply":"2025-11-08T18:56:29.721314Z"}},"outputs":[{"name":"stdout","text":"Reranking → device=cuda, topk=20, pred_batch=32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reranking (CUDA):   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09228c7d1ad74fd68a40190500616b4d"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'R@10': 0.2589, 'MRR@10': 0.22455869047619045, 'nDCG@10': 0.23293820572715604}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import json, os\n\nos.makedirs(\"outputs\", exist_ok=True)\nmetrics = {\n    \"bi\": {\n        \"R@10\": float(bi_R10),\n        \"MRR@10\": float(bi_MRR10),\n        \"nDCG@10\": float(bi_nDCG10),\n    },\n    \"pipeline\": {\n        \"R@10\": float(pipe_R10),\n        \"MRR@10\": float(pipe_MRR10),\n        \"nDCG@10\": float(pipe_nDCG10),\n    },\n}\nwith open(\"outputs/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metrics, f, ensure_ascii=False, indent=2)\nmetrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:01:02.811086Z","iopub.execute_input":"2025-11-08T19:01:02.811423Z","iopub.status.idle":"2025-11-08T19:01:02.818797Z","shell.execute_reply.started":"2025-11-08T19:01:02.811400Z","shell.execute_reply":"2025-11-08T19:01:02.818200Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'bi': {'R@10': 0.2497,\n  'MRR@10': 0.2036152380952377,\n  'nDCG@10': 0.2147046798742635},\n 'pipeline': {'R@10': 0.2589,\n  'MRR@10': 0.22455869047619045,\n  'nDCG@10': 0.23293820572715604}}"},"metadata":{}}],"execution_count":21}]}